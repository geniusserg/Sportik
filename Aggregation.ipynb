{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "raising-january",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/opt/hadoop-2.7.2/share/hadoop/common/lib/hadoop-auth-2.7.2.jar) to method sun.security.krb5.Config.getInstance()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Found 6 items\n",
      "-rw-r--r--   3 kzaharov-369865 kzaharov-369865   28004109 2023-06-17 09:30 /home/kzaharov-369865/games.csv\n",
      "-rw-r--r--   3 kzaharov-369865 kzaharov-369865         12 2023-05-22 08:01 /home/kzaharov-369865/hello-file.txt\n",
      "-rw-r--r--   3 kzaharov-369865 kzaharov-369865   84908201 2023-06-16 16:35 /home/kzaharov-369865/players.csv\n",
      "-rw-r--r--   3 kzaharov-369865 kzaharov-369865  360088309 2023-06-07 20:06 /home/kzaharov-369865/players_new.csv\n",
      "-rw-r--r--   3 kzaharov-369865 kzaharov-369865    2526182 2023-06-16 16:36 /home/kzaharov-369865/teams.csv\n",
      "-rw-r--r--   3 kzaharov-369865 kzaharov-369865       2595 2023-05-22 11:03 /home/kzaharov-369865/test.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /home/kzaharov-369865/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "introductory-thompson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/opt/hadoop-2.7.2/share/hadoop/common/lib/hadoop-auth-2.7.2.jar) to method sun.security.krb5.Config.getInstance()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/06/17 09:31:39 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /home/kzaharov-369865/players1.csv\n"
     ]
    }
   ],
   "source": [
    "# !hdfs dfs -rm -R /home/kzaharov-369865/players_new.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "advanced-sarah",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import types as T, functions as F, SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark import StorageLevel\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "LOGIN = \"kzaharov-369865\"  # Your gateway.st login\n",
    "APP_NAME = \"2\"  # Any name for your Spark-app\n",
    "\n",
    "NORMALIZED_APP_NAME = APP_NAME.replace('/', '_').replace(':', '_').replace(' ', '_').replace('\\\\', '_')\n",
    "\n",
    "APPS_TMP_DIR = os.path.join(os.getcwd(), \"tmp\")\n",
    "APPS_CONF_DIR = os.path.join(os.getcwd(), \"conf\")\n",
    "APPS_LOGS_DIR = os.path.join(os.getcwd(), \"logs\")\n",
    "LOG4J_PROP_FILE = os.path.join(APPS_CONF_DIR, \"pyspark-log4j-{}.properties\".format(NORMALIZED_APP_NAME))\n",
    "LOG_FILE = os.path.join(APPS_LOGS_DIR, 'pyspark-{}.log'.format(NORMALIZED_APP_NAME))\n",
    "EXTRA_JAVA_OPTIONS = (\n",
    "    \"-Dlog4j.configuration=file://{} \"\n",
    "    \"-Dspark.hadoop.dfs.replication=1 \"\n",
    "    \"-Dhttps.protocols=TLSv1.0,TLSv1.1,TLSv1.2,TLSv1.3\"\n",
    "    .format(LOG4J_PROP_FILE)\n",
    ")\n",
    "\n",
    "LOCAL_IP = socket.gethostbyname(socket.gethostname())\n",
    "\n",
    "for directory in [APPS_CONF_DIR, APPS_LOGS_DIR, APPS_TMP_DIR]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "env = Environment(loader=FileSystemLoader('/opt'))\n",
    "template = env.get_template(\"pyspark_log4j.properties.template\")\n",
    "template.stream(logfile=LOG_FILE).dump(LOG4J_PROP_FILE)\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(APP_NAME)\n",
    "    \n",
    "    # Master URI/configuration\n",
    "    .master(\"k8s://https://10.32.7.103:6443\")\n",
    "    \n",
    "    .config(\"spark.driver.host\", LOCAL_IP)\n",
    "    \n",
    "    # Web-UI port for your Spark-app\n",
    "    .config(\"spark.ui.port\", \"4040\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    \n",
    "    # How many CPU cores allocate to driver process\n",
    "    .config(\"spark.driver.cores\", \"2\")\n",
    "    \n",
    "    # How many RAM allocate to driver process\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    \n",
    "    # How many executors to create\n",
    "    .config(\"spark.executor.instances\", \"3\")\n",
    "    \n",
    "    # How many CPU cores allocate to each executor\n",
    "    .config(\"spark.executor.cores\", '2')\n",
    "    \n",
    "    # How many RAM allocate to each executor\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    \n",
    "    # How many extra RAM allocate to each executor pod to handle with JVM overheads\n",
    "    # Total pod RAM = 'spark.executor.memory' + ('spark.executor.memory' * 'spark.kubernetes.memoryOverheadFactor')\n",
    "    .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.2\")\n",
    "    \n",
    "    # How many RAM from the pool allocate to store the data\n",
    "    # Additional info: https://spark.apache.org/docs/latest/tuning.html#memory-management-overview\n",
    "    .config(\"spark.memory.fraction\", \"0.6\")\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\")\n",
    "    \n",
    "    .config(\"spark.network.timeout\", \"180s\")\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", EXTRA_JAVA_OPTIONS)\n",
    "    \n",
    "    # Namespace to create executor pods. You are allowed to create pods only in your own namespace\n",
    "    .config(\"spark.kubernetes.namespace\", LOGIN)\n",
    "    \n",
    "    # Extra labels to your driver/executor pods in Kubernetes\n",
    "    .config(\"spark.kubernetes.driver.label.appname\", APP_NAME)\n",
    "    .config(\"spark.kubernetes.executor.label.appname\", APP_NAME)\n",
    "    \n",
    "    # Spark executor image\n",
    "    .config(\"spark.kubernetes.container.image\", f\"node03.st:5000/spark-executor:{LOGIN}\")\n",
    "\n",
    "    .config(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "    \n",
    "    # If true - delete completed/failed pods. \n",
    "    # If your executors goes down you can set 'false' to check logs and troubleshoot your app.\n",
    "    .config(\"spark.kubernetes.executor.deleteOnTermination\", \"true\")\n",
    "    \n",
    "    .config(\"spark.local.dir\", \"/tmp/spark\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "forbidden-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !hdfs dfs -cat /home/kzaharov-369865/games.zip | gzip -d | hdfs dfs -put - /home/kzaharov-369865/games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "detailed-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip hdfs dfs -cat /home/kzaharov-369865/games.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-migration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "amazing-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-camcorder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pleased-anniversary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.load('/home/kzaharov-369865/players_new.csv', format='csv',\\\n",
    "#                      chunksize=10, header=True, maxColumns=98000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "musical-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "complex-component",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select(F.col('height')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "twelve-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = (F.col('height'), 'firstName', 'active', 'jersey', 'id', 'weight', 'dateOfBirth', 'age', 'slug', 'debutYear',\\\n",
    "#        F.col('collegeAthlete.draft.round'), F.col('collegeAthlete.draft.year'), 'collegeAthlete.draft.selection',\\\n",
    "#        'status.abbreviation', 'experience.years',\\\n",
    "#        'position.name', 'position.id', 'team.id.team', 'team.start.season', 'draft.team.id.team',\\\n",
    "#        'college.shortName', 'collegeAthlete.jersey', 'collegeAthlete.height', 'collegeAthlete.weight',\\\n",
    "#        'position.parent.abbreviation', 'collegeAthlete.birthPlace.city',\\\n",
    "#        'statistics.splits.categories.1.stats.0.value', 'statistics.splits.categories.1.stats.0.displayValue',\\\n",
    "#        'statistics.splits.categories.1.stats.37.displayValue')\n",
    "\n",
    "# df = df.select(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "negative-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "surprised-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.load('/home/kzaharov-369865/test.csv', format='csv', header=False)\n",
    "# df = spark.read.csv('/home/kzaharov-369865/test.csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "found-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = spark.read.csv('/home/kzaharov-369865/teams.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "otherwise-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_comma_to_dot_names(s: str) -> str:\n",
    "    split_s = list(s)\n",
    "    flag = False\n",
    "    res = s\n",
    "    \n",
    "    for i in split_s:\n",
    "        if i == '.':\n",
    "            flag = True\n",
    "            \n",
    "    if flag:\n",
    "        res = '`' + ''.join(list(s)) + '`'\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "cooperative-oxide",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_dot_columns(columns: str) -> str:\n",
    "    new_columns = []\n",
    "    \n",
    "    for col in columns:\n",
    "        flag = False\n",
    "        m = [0]\n",
    "        \n",
    "        split_col = list(col)\n",
    "        for i in range(len(split_col)):\n",
    "            if split_col[i] == '.':\n",
    "                flag = True\n",
    "                m.append(i)\n",
    "        m.append(len(split_col))\n",
    "        \n",
    "        if flag:\n",
    "            temp = []\n",
    "            for j in range(len(m) - 1):\n",
    "                temp.append(split_col[m[j]:m[j+1]])\n",
    "            \n",
    "            temp2 = [''.join(temp[0])]\n",
    "            for i in temp[1:]:\n",
    "                i.pop(0)\n",
    "                temp2.append(''.join(i))\n",
    "            rename_col = ''.join(temp2)\n",
    "            \n",
    "        else:\n",
    "            rename_col = col\n",
    "            \n",
    "        new_columns.append(rename_col)\n",
    "\n",
    "    return new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "shaped-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_columns(data, ths: list = [31, 1]):\n",
    "\n",
    "    float_attr = np.where(np.array(data.dtypes).T[1] == 'double')[0]\n",
    "    int_attr = np.where(np.array(data.dtypes).T[1] == 'int')[0]\n",
    "    bool_attr= np.where(np.array(data.dtypes).T[1] == 'boolean')[0]\n",
    "    str_attr = np.where(np.array(data.dtypes).T[1] == 'string')[0]\n",
    "    \n",
    "\n",
    "    if np.any(float_attr):\n",
    "        float_columns = np.array(data.dtypes)[float_attr].T[0]\n",
    "    else:\n",
    "        float_columns = np.array([])\n",
    "\n",
    "    if np.any(int_attr):\n",
    "        int_columns = np.array(data.dtypes)[int_attr].T[0]\n",
    "    else:\n",
    "        int_columns = np.array([])\n",
    "\n",
    "    if np.any(bool_attr):\n",
    "        bool_columns = np.array(data.dtypes)[bool_attr].T[0]\n",
    "    else:\n",
    "        bool_columns = np.array([])\n",
    "\n",
    "    if np.any(str_attr):\n",
    "        str_columns = np.array(data.dtypes)[str_attr].T[0]\n",
    "    else:\n",
    "        str_columns = np.array([])\n",
    "        \n",
    "        \n",
    "    float_columns = list(map(lambda x: add_comma_to_dot_names(x), float_columns))\n",
    "    int_columns = list(map(lambda x: add_comma_to_dot_names(x), int_columns))\n",
    "    bool_columns = list(map(lambda x: add_comma_to_dot_names(x), bool_columns))\n",
    "    str_columns = list(map(lambda x: add_comma_to_dot_names(x), str_columns))\n",
    "    \n",
    "    #float attributes\n",
    "    data_float_cols = data.select(*float_columns)\n",
    "    data_float_cols_describe = data_float_cols.describe()\n",
    "    \n",
    "    data_float_cols_std = data_float_cols_describe.filter(F.col('summary') == 'stddev')\n",
    "    data_float_cols_count = data_float_cols_describe.filter(F.col('summary') == 'count')\n",
    "\n",
    "    data_float_cols_std_dict = data_float_cols_std.collect()[0].asDict()\n",
    "    data_float_cols_count_dict = data_float_cols_count.collect()[0].asDict()\n",
    "\n",
    "    del data_float_cols_std_dict['summary']\n",
    "    del data_float_cols_count_dict['summary']\n",
    "    \n",
    "    f1 = [key for (key, value) in data_float_cols_std_dict.items() if value != '0.0']\n",
    "    f2 = [key for (key, value) in data_float_cols_count_dict.items() if int(value) >= ths[0]]\n",
    "    non_const_float = np.intersect1d(f1, f2)\n",
    "    \n",
    "    #integer attributes\n",
    "    data_int_cols = data.select(*int_columns)\n",
    "    data_int_cols_describe = data_int_cols.describe()\n",
    "    \n",
    "    data_int_cols_std = data_int_cols_describe.filter(F.col('summary') == 'stddev')\n",
    "    data_int_cols_count = data_int_cols_describe.filter(F.col('summary') == 'count')\n",
    "\n",
    "    data_int_cols_std_dict = data_int_cols_std.collect()[0].asDict()\n",
    "    data_int_cols_count_dict = data_int_cols_count.collect()[0].asDict()\n",
    "\n",
    "    del data_int_cols_std_dict['summary']\n",
    "    del data_int_cols_count_dict['summary']\n",
    "    \n",
    "    i1 = [key for (key, value) in data_int_cols_std_dict.items() if value != '0.0']\n",
    "    i2 = [key for (key, value) in data_int_cols_count_dict.items() if int(value) >= ths[1]]\n",
    "    non_const_int = np.intersect1d(i1, i2)\n",
    "        \n",
    "    #string attributes\n",
    "    counts = []\n",
    "    for i in str_columns:\n",
    "        counts.append([i, data.select(i).distinct().count()])\n",
    "        \n",
    "    non_const_str = np.array(counts).T[0][np.where(np.array(counts).T[1].astype(int) > 1)[0]]\n",
    "\n",
    "    \n",
    "    #boolean attributes\n",
    "    data_bool_cols = data.select(*bool_columns)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #FILL NAN VALUES\n",
    "    \n",
    "    #Float\n",
    "    data_float = data.select(*list(map(lambda x: add_comma_to_dot_names(x), non_const_float)))\n",
    "    non_const_float = rename_dot_columns(non_const_float) \n",
    "    data_float = data_float.toDF(*non_const_float)\n",
    "    \n",
    "    fill_float = Imputer(\n",
    "            inputCols = data_float.columns,\n",
    "            outputCols = data_float.columns\n",
    "        ).setStrategy(\"median\")\n",
    "    data_float_clean = fill_float.fit(data_float).transform(data_float)\n",
    "    \n",
    "    \n",
    "    #Integer\n",
    "    data_int = data.select(*list(map(lambda x: add_comma_to_dot_names(x), non_const_int)))\n",
    "    non_const_int = rename_dot_columns(non_const_int) \n",
    "    data_int = data_int.toDF(*non_const_int)\n",
    "    \n",
    "    fill_int = Imputer(\n",
    "            inputCols = data_int.columns,\n",
    "            outputCols = data_int.columns\n",
    "        ).setStrategy(\"median\")\n",
    "    data_int_clean = fill_int.fit(data_int).transform(data_int)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     data.loc[:, non_const_float_players] = data[non_const_float_players].fillna(data[non_const_float_players].median().fillna(0))\n",
    "#     data.loc[:, non_const_int_players] = data[non_const_int_players].fillna(data[non_const_int_players].median().fillna(0))\n",
    "#     data.loc[:, non_const_str_players] = data[non_const_str_players].fillna(data[non_const_str_players].describe().loc['top'])\n",
    "\n",
    "#     data = data[np.hstack([non_const_float_players, non_const_int_players, bool_columns_players, non_const_str_players])]\n",
    "\n",
    "\n",
    "\n",
    "#     data = data_float_clean.unionByName(data_int_clean)\n",
    "\n",
    "    cols = np.hstack([data_float_clean.columns, data_int_clean.columns])\n",
    "    data = pd.concat([data_float_clean.toPandas(), data_int_clean.toPandas(), data_bool_cols.toPandas()], axis=1)\n",
    "    data = spark.createDataFrame(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "described-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_teams = clear_columns(teams)\n",
    "clean_teams = clean_teams.drop('statistics.season.$ref', 'statistics.seasonType.$ref', 'displayName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "chubby-buddy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 465\n"
     ]
    }
   ],
   "source": [
    "print(clean_teams.count(), len(clean_teams.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "floral-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_teams = clean_teams.withColumnRenamed('id', 'team_id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "specific-bacteria",
   "metadata": {},
   "source": [
    "## Clear players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "southern-support",
   "metadata": {},
   "outputs": [],
   "source": [
    "players = spark.read.csv('/home/kzaharov-369865/players.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "forward-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_players = clear_columns(players, ths=[10000, 31])\n",
    "clean_players = clean_players.drop('Unnamed: 0', 'college.id', 'collegeAthlete.college.id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "resident-belize",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17748 231\n"
     ]
    }
   ],
   "source": [
    "print(clean_players.count(), len(clean_players.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "bigger-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_players = clean_players.withColumnRenamed('teamidteam', 'team_id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "flexible-civilian",
   "metadata": {},
   "source": [
    "## Merge players with teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "prescribed-alliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_players = clean_players.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "organizational-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_teams = clean_teams.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "endless-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_w_teams = pd.merge(pd_players, pd_teams, on=['team_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "charged-relations",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(556288, 695)"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players_w_teams.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "racial-cleveland",
   "metadata": {},
   "source": [
    "## Merge teams with games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "moderate-buddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "games = spark.read.csv('/home/kzaharov-369865/games.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "valuable-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "games = clear_columns(games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "varying-imperial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15936 335\n"
     ]
    }
   ],
   "source": [
    "print(games.count(), len(games.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(players_w_teams, games.toPandas(), on=['team_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('matches.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-matthew",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-alignment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
